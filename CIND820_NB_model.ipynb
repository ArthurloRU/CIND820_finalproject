{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\losh\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.7.0-cp38-cp38-win_amd64.whl (430.8 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-12.0.0-py2.py3-none-win_amd64.whl (13.1 MB)\n",
      "Collecting wheel<1.0,>=0.32.0\n",
      "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.0 in c:\\users\\losh\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.19.1-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.5 in c:\\users\\losh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (1.21.2)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp38-cp38-win_amd64.whl (2.8 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\users\\losh\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.42.0-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied, skipping upgrade: keras<2.8,>=2.7.0rc0 in c:\\users\\losh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\losh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (47.1.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Downloading charset_normalizer-2.0.8-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting idna<4,>=2.5; python_version >= \"3\"\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Using legacy setup.py install for termcolor, since package 'wheel' is not installed.\n",
      "Installing collected packages: opt-einsum, gast, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, charset-normalizer, certifi, idna, urllib3, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, werkzeug, tensorboard-plugin-wit, zipp, importlib-metadata, markdown, protobuf, wheel, tensorboard-data-server, absl-py, tensorboard, libclang, tensorflow-estimator, google-pasta, termcolor, tensorflow-io-gcs-filesystem, typing-extensions, astunparse, h5py, flatbuffers, keras-preprocessing, tensorflow\n",
      "    Running setup.py install for termcolor: started\n",
      "    Running setup.py install for termcolor: finished with status 'done'\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.8 flatbuffers-2.0 gast-0.4.0 google-auth-2.3.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.42.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.8.2 keras-preprocessing-1.1.2 libclang-12.0.0 markdown-3.3.6 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.22.0 termcolor-1.1.0 typing-extensions-4.0.0 urllib3-1.26.7 werkzeug-2.0.2 wheel-0.37.0 zipp-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\losh\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U keras\n",
    "!pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, GRU\n",
    "from keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "# from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 mil reviews\n",
    "import csv\n",
    "import pandas as pd\n",
    "df_review_csv = pd.read_csv('D:\\Ryerson\\CIND820\\Kaggle_yelp_dataset_CSV\\yelp_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#170k businesses\n",
    "import csv\n",
    "import pandas as pd\n",
    "df_business_csv = pd.read_csv('D:\\Ryerson\\CIND820\\Kaggle_yelp_dataset_CSV\\yelp_business.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_business_csv[df_business_csv['categories'].str.contains('Restaurant') == True]\n",
    "rev = df_review_csv[df_review_csv.business_id.isin(a['business_id']) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_samp = rev.sample(n = 350000, random_state = 42)\n",
    "train = rev_samp[0:280000]\n",
    "test = rev_samp[280000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((280000, 9), (70000, 9))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2760442</th>\n",
       "      <td>Second time here.... first time had the pulled...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014452</th>\n",
       "      <td>Great place. Like their sauce and lunch specia...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876979</th>\n",
       "      <td>So goooooooood and so simple! I love their pel...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469097</th>\n",
       "      <td>We stopped in for a late lunch on a Tuesday af...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971248</th>\n",
       "      <td>A great option to try hakka chinese since its ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  stars\n",
       "2760442  Second time here.... first time had the pulled...      5\n",
       "3014452  Great place. Like their sauce and lunch specia...      5\n",
       "2876979  So goooooooood and so simple! I love their pel...      5\n",
       "469097   We stopped in for a late lunch on a Tuesday af...      3\n",
       "4971248  A great option to try hakka chinese since its ...      4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train = pd.read_csv('/home/adam/R/Yelp/dataset/model_train.csv', usecols = ['text', 'stars'])\n",
    "train = train[['text', 'stars']]\n",
    "train['stars'].hist();train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars_1</th>\n",
       "      <th>stars_2</th>\n",
       "      <th>stars_3</th>\n",
       "      <th>stars_4</th>\n",
       "      <th>stars_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2760442</th>\n",
       "      <td>Second time here.... first time had the pulled...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014452</th>\n",
       "      <td>Great place. Like their sauce and lunch specia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876979</th>\n",
       "      <td>So goooooooood and so simple! I love their pel...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469097</th>\n",
       "      <td>We stopped in for a late lunch on a Tuesday af...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971248</th>\n",
       "      <td>A great option to try hakka chinese since its ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  stars_1  stars_2  \\\n",
       "2760442  Second time here.... first time had the pulled...        0        0   \n",
       "3014452  Great place. Like their sauce and lunch specia...        0        0   \n",
       "2876979  So goooooooood and so simple! I love their pel...        0        0   \n",
       "469097   We stopped in for a late lunch on a Tuesday af...        0        0   \n",
       "4971248  A great option to try hakka chinese since its ...        0        0   \n",
       "\n",
       "         stars_3  stars_4  stars_5  \n",
       "2760442        0        0        1  \n",
       "3014452        0        0        1  \n",
       "2876979        0        0        1  \n",
       "469097         1        0        0  \n",
       "4971248        0        1        0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.get_dummies(train, columns = ['stars'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((280000, 6), (70000, 6))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test = pd.read_csv('/home/adam/R/Yelp/dataset/model_test.csv', usecols=['text', 'stars'])\n",
    "test = test[['text', 'stars']]\n",
    "test = pd.get_dummies(test, columns = ['stars'])\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28000, 6), (7000, 6))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set frac = 1. to use the entire sample\n",
    "train_samp = train.sample(frac = .1, random_state = 42)\n",
    "test_samp = test.sample(frac = .1, random_state = 42)\n",
    "train_samp.shape, test_samp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features is an upper bound on the number of words in the vocabulary\n",
    "max_features = 2000\n",
    "tfidf = TfidfVectorizer(max_features = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBFeatures(BaseEstimator):\n",
    "    '''Class implementation of Jeremy Howards NB Linear model'''\n",
    "    def __init__(self, alpha):\n",
    "        # Smoothing Parameter: always going to be one for my use\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def preprocess_x(self, x, r):\n",
    "        return x.multiply(r)\n",
    "    \n",
    "    # calculate probabilities\n",
    "    def pr(self, x, y_i, y):\n",
    "        p = x[y == y_i].sum(0)\n",
    "        return (p + self.alpha)/((y==y_i).sum()+self.alpha)\n",
    "    \n",
    "    # calculate the log ratio and represent as sparse matrix\n",
    "    # ie fit the nb model\n",
    "    def fit(self, x, y = None):\n",
    "        self._r = sparse.csr_matrix(np.log(self.pr(x, 1, y) /self.pr(x, 0, y)))\n",
    "        return self\n",
    "    \n",
    "    # apply the nb fit to original features x\n",
    "    def transform(self, x):\n",
    "        x_nb = self.preprocess_x(x, self._r)\n",
    "        return x_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline using sklearn pipeline:\n",
    "    # I basically create my tfidf features which are fed to my NB model \n",
    "    # for probability calculations. Then those are fed as input to my \n",
    "    # logistic regression model.\n",
    "lr = LogisticRegression()\n",
    "nb = NBFeatures(1)\n",
    "p = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('nb', nb),\n",
    "    ('lr', lr)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class stars_1 is 0.9282499819604656\n",
      "CV score for class stars_2 is 0.90339283521352\n",
      "CV score for class stars_3 is 0.8591786654537303\n",
      "CV score for class stars_4 is 0.7321071676830603\n",
      "CV score for class stars_5 is 0.8044644727087923\n"
     ]
    }
   ],
   "source": [
    "class_names = ['stars_1', 'stars_2', 'stars_3', 'stars_4', 'stars_5']\n",
    "scores = []\n",
    "preds = np.zeros((len(test_samp), len(class_names)))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    train_target = train_samp[class_name]    \n",
    "    cv_score = np.mean(cross_val_score(estimator = p, X = train_samp['text'].values, \n",
    "                                      y = train_target, cv = 3, scoring = 'accuracy'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "    p.fit(train_samp['text'].values, train_target)\n",
    "    preds[:,i] = p.predict_proba(test_samp['text'].values)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.73      0.68       771\n",
      "           1       0.43      0.15      0.23       687\n",
      "           2       0.49      0.31      0.38      1020\n",
      "           3       0.47      0.46      0.47      1906\n",
      "           4       0.67      0.85      0.75      2616\n",
      "\n",
      "    accuracy                           0.59      7000\n",
      "   macro avg       0.54      0.50      0.50      7000\n",
      "weighted avg       0.56      0.59      0.56      7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = metrics.classification_report(np.argmax(test_samp[class_names].values, axis = 1),np.argmax(preds, axis = 1))\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8307cd9af0e97630f8d6fae8fa8cf075c5a8326acb952193157434f24c616b4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
